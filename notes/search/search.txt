Lecture 0: Search

Search problems involve an agent that is given an initial state and a goal state, and it returns a solution of how to get
from the former to the latter. A navigator app uses a typical search process, where the agent receives your current location
and desired location as input, then returns a suggested path based on a search algorithm. However, there are many other 
forms of search problems, like puzzles or mazes. 

Finding a solution to a 15 puzzle, for example, would require the use of a search algorithm.

Defs
----

--> Agent: An entity that perceives its environment and acts upon that environment. In a navigator app, the agent 
would be a representation of a car that needs to decide on which actions to take to arrive at the destination.

--> State: A configuration of an agent in its environment. In a 15 puzzle, a state is any one way that all the #s
are arranged.
    --> Initial State: State from which the search algorithm starts (current location for navigator app)

--> Actions: Choices that can be made in a state. More precisely, a function Actions(s) that receives a state s 
and returns the set of executable actions. For example, in a 15 puzzle, the actions of a given state are the ways
you can slide squares in the current configuration. 

--> Transition Model: A description of what state results from performing any applicable action in any state. 
More precisely, a function Results(s,a) takes a state and an action as input and returns the resulting state. 
In a 15 puzzle (state s) moving a square in any direction (action a) will bring the puzzle to a new configuration.

--> State Space: The set of all states reachable from the initial state by any sequence of actions. For example, in a 
15 puzzle, state space consists of 16!/2 configurations on the board that can be reached from any initial state.
The state space can be visualized as a directed graph with states, represented as nodes, and actions, represented 
as arrows between nodes.

--> Goal Test: The condition that determines whether a given state is a goal state. For example, in a navigator app,
the goal test would be whether the current location of the agent (the representation of the car) is at the 
destination. If it is, good, if not, continue searching.

--> Path Cost: A numerical cost associated with a given path. For example, a navigator app does not bring  you to your 
goal, it does so while minimizing the path cost (fastest way possible to get to goal state).


-- SOLVING SEARCH PROBLEMS 

The solution to a search problem is a sequence of actions that leads from the initial state to the goal state.
The OPTIMAL solution is the solution that has the lowest path cost.

In a search process, data is often stored in a NODE, which is a data structure containing:
    1. A state 
    2. Its parent node, through which the current node was generated
    3. The action that was applied to the parent note to get to the current node
    4. The path cost from the initial state to this node 

Nodes contain info that make them very useful in search algorithms. They contain a state, which can be checked
using the goal test to see if it is the final state. If it is, the node's path cost can be compared to the path cost
of other nodes, which allows choosing the optimal solution. Once the node is chosen, by virtue of storing 
the parent node and the action that led to it, its possible to trace back every step
of the way from the initial state to this node. This sequence of actions is the solution.

However, nodes are simply a data structure -- they don't search, but merely hold info. To actually search, we use the 
FRONTIER, the mechanism that manages the nodes. The frontier starts by containing an initial state and an empty set of 
explored items, and then repeats the following actions until a solution is reached: 

    APPROACH
    1. If the frontier is empty, STOP. No solution to problem.
    2. Remove a node from the frontier. This is the node that will be considered. 
    3. If this node contains the goal state, return the solution and stop. 
    Otherwise, expand the node (find all new nodes that could be reached) and add resulting nodes to the frontier.
    Add the current node to the explored set. 

-- Depth-First Search 

Which nodes should be considered first? 

In the depth-first search (DFS) approach, the frontier is managed as a stack data structure, i.e "last-in first-out". That is,
DFS algorithms exhaust each one direction before trying another direction. The first node to remove from the frontier 
and consider is the last one that was added to the frontier. This goes as deep as possible in one way, 
and leaves other directions for later.

PROS 
1. At best, fastest, if it lucks out and chooses the right path by chance. May save memory.

CONS 
1. Found solution may not be optimal
2. At worst, slowest, as it will explore every possible path before even getting to the correct one
    
-- Breadth-First Search

Search algorithm that always expands the shallowest node in the frontier, frontier uses the queue data structure,
which is a first-in first-out data type. Opposite of the depth-first search, the breadth-first search always
expands the oldest frontier node.

Solution will be optimal, as it is the shallowest path. However, excess computational time is used for paths
that end up not being used.

< TRANSITION TO CODE MAZE.PY > 

Can we make this algorithm more intelligent? In other words, can we make 
its choices be closer to human intuition?

-- Uninformed Search 

Search strategy that uses no problem-specific knowledge

-- Informed Search 

Search strategy that uses problem-specific knowledge to find solutions
more efficiently. Eg in a maze, being in a position closer to the goal
is generally good.

TYPES:

1. Greedy best-first search (GBFS): Search algorithm that expands the
node that is closest to the goal, as estimated by a heuristic function
h(n)

    What can h(n) actually look like?

    In the case of a maze, it needs to answer the question: between
    cells A and B, which is better? Where should I be?

    Like a human, it should recognize that, via coordinate pairs, B is 
    closer to the goal. So h(n) in this case could be the "Manhattan distance"
    (distance using left/right and up/down movement). 

    In practice, whenever there is a decision between two nodes, it chooses 
    the node with the smaller heuristic.

    **Effectiveness is based on the effectiveness of the heuristic, which 
    is often tricky to determine!

    "Greedy" because it only looks locally, it doesnt consider the whole
    picture.

2. A* search: Search algorithm that expands the node with the lowest
value of g(n) + h(n), where
g(n) = cost to reach node 
h(n) = estimated cost to goal 

In the maze, this algorithm considers that taking 5 steps to get to a 13 
is better than taking 10 steps to get to a 6. So once it reaches that 6,
it will back track!

Optimal if:
    1. h(n) is admissible (never overestimates the true cost)
    2. h(n) is consistent (for every node n and successing node n'
    with step cost c, h(n) <= h(n') + c)

Pretty good algorithm -- IF the heuristic is good!


-- Adversarial Search 

There is often more than one agent. It is common to have an adversary,
an agent trying to prevent us from achieving a goal -- think of tic tac 
toe AIs, chess AIs, etc. Adversarial search algorithms take this into
account.

1. Minimax: MAX player aims to maximize the score, while the MIN player
aims to minimize the score.

    In tic tac toe, assume O winning is score=-1. Tie score=1. X win
    score=1.
    In this case, X is the max player, since the maximum score is them
    winning.

    To make a tic-tac-toe game, we need:

        S0: initial state
        Player(s): function that returns which players turn it is in state
        s 
        Actions(s): returns legal states in move s 
        Result(s, a): returns state after action a is taken in state s 
        Terminal(s): checks if the state s is terminal (game over)
        Utility(s): final numerical value for terminal state s 

        Minimax is recursive. If there are multiple moves, it considers
        all moves, the states resulting from those moves, and then what
        X will do to maximize the score. 

        If a particular move will only lead to X winning, then that 
        board has a value of 1. 

        Eg if there are two moves and one action leads to a board with
        value 1, and the other leads to value 0, then of course, the min
        player will choose the latter.


        PSEUDOCODE:

        Given a state s:

            MAX picks action a in Actions(s) that produces highest
            value of Min-Value(Results(s, a))
            MIN picks action a in Actions(s) that produces smallest 
            value of Max-Value(Results(s, a))

        function Max-Value(s):
            if terminal(s):
                return Utility(s)
            v = -inf 
            for action in Actions(s):
                v = max(v, Min-Value(Result(s, a)))
            return v

        function Min-Value(s): 
            if terminal(s):
                return Utility(s)
            v = inf
            for action in Actions(s):
                v = min(v, Max-Value(Result(s, a)))
            return v

        This can definitely get long.. imagine chess! What kind of 
        optimizations can be made?

        Alpha-Beta Pruning can neglect nodes that will not change the 
        decision anyway. For example, consider max player has the option 
        of boards A and B. Board A has a value of 4, which was calculated.
        Board B's score will depend on the min players moves, so those must
        be considered. The first min player option leads to a value 5, 
        the second leads to a value 3, and the third leads to a value 8. 
        As soon as this 3 is calculated, we know that this boards value is 
        AT MOST 3. Why bother checking the last node if we already know the
        first board is better?

        But this doesnt work for extremely deep games. Just consider chess,
        with over 10^29000 possible games. In this case, we need...

        -- Depth-limited Minimax: Moves further than some cap, say, 10
        moves, wont be considered.

            How are values calculated then, if we dont know the value
            a move leads to?

            --> evaluation function: function that estimates the expected
            utility of the game from a given state.

                e.g chess --> value = 0.8 means white is likely to win,
                but not guaranteed.

    
